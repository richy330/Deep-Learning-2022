
\section{Problem description: Neural Networks: Regression}
The objective of this assignment is to design a regression problem using the provided dataset on social capital. The dataset was downloaded from the following website:

https://data.humdata.org/dataset/social-capital-atlas

This regression problem will be modeled with a neural network using the Python libraries Tensorflow 2.0 and Keras. Different settings (architecture, optimizer, activation, ...) of the neural network will be explored to find the best fit for the chosen dataset.

\subsection*{1.1 Preprocessing of the raw data}

The first task of the practical is to choose a dataset, and from that create separate training- and test datasets. The training set should contain 80 \% of the data and the test data 20 \%. The chosen dataset is ’social\_capital\_zip.csv’. We decided to normalize the data, as suggested in the assignment, to values between zero and one with the following equation:
\[x_{i,normalized} = \frac{x_{i}-x_{min}}{x_{max}-x_{min}}\]

\subsection*{1.2 Definition of the regression problem}
For designing the regression problem, input and output variables must be chosen. For the input data, the following set of labels was chosen: 
\begin{itemize}
\item ec\_zip
\item ec\_se\_zip
\item nbhd\_ec\_zip
\item ec\_grp\_mem\_zip
\item ec\_high\_zip
\item ec\_high\_se\_zip
\item nbhd\_ec\_high\_zip
\item ec\_grp\_mem\_high\_zip
\item exposure\_grp\_mem\_zip
\item exposure\_grp\_mem\_high\_zip
\item nbhd\_exposure\_zip
\item bias\_grp\_mem\_zip
\item bias\_grp\_mem\_high\_zip
\item nbhd\_bias\_zip
\item nbhd\_bias\_high\_zip
\item clustering\_zip
\item support\_ratio\_zip
\end{itemize}

The reason why we choose almost all of the given labels is that with a larger amount of information, the neural network could theoretically detect statistical patterns easier, for which it is not trivial to understand whether they affect the output parameters. Only the "zip", "county", "num\_below\_p50" and "pop2018" data were not included because we assume that these labels (e.g., the number of counties) do not affect the social metrics, and are more to be seen as an artificial index, that should not have any effect on social-economic status.

The output or prediction variables we chose are:
\begin{itemize}
  \item volunteering\_rate\_zip
  \\This variable describes the percentage of Facebook users who are members of a group that is assumed to be about ‘volunteering’ or ‘activism’ based on the group title and other group characteristics.
  \item civic\_organizations\_zip
  \\The second variable describes the number of Facebook pages predicted to be "common good" pages based on page title, category, and other page characteristics, per 1,000 users in the zip code.
\end{itemize}

\subsection*{1.3 Architecture and activation of the neural network}
20 \% of the raw data was chosen randomly to be the test set. The remaining 80\% were again split into 20 \% validation set and 80 \% training set. 
For all our evaluations the batch size was set to 64 and the network was trained over 50 epochs. We decided to investigate three different neural network structures and two different activation functions. The structures are: 
\begin{itemize}
 \item $[17, 32, 8, 32, 2]$
 \item $[17, 32, 32, 2]$
 \item $[17, 8, 8, 2]$
\end{itemize}

Where each entry in the lists corresponds to the number of neurons of one layer, with the leftmost entry corresponding to the input layer, the rightmost entry corresponding to the output layer, and the other entries corresponding to hidden layers.
The number of input neurons was fixed for all inputs, since we have the same amount of input data for all variants. The same is true for the output layer which always contains two neurons. 
The following activation functions were investigated:

\begin{itemize}
	\item ReLU 
	\item sigmoid
\end{itemize} 

\subsection*{1.4 Optimizers and learning rates of the neural network}
Learning rates were investigated with static values and also with exponential decay.
The value of the static learning rate was $10^{-4}$.

Values for Exponential Decay were:
\begin{itemize}
	\item $initial\ learing\ rate =  10^{-3}$, 
	\item $decay\ steps=20$, 
	\item $decay\ rate=0.9$
\end{itemize}



Three different optimizers were investigated: 
\begin{itemize}
	\item ADAM
	\item SGD
	\item SGD Momentum
\end{itemize}

This results in a total number of 36 investigated models.

The momentum factor for the SGD Momentum was $0.5$.

Table \ref{table:overview_table} shows all the investigated combinations of optimizers, network structures, activation function and learning rates, along with the respective loss on the validation set. The table is ordered from lowest loss (best model) to highest loss (worst model). It appears that the Adam optimizer generaly performs best, since all hyperparameter combinations with this optimizer are among the best performers, and no architecture with it appears in the lower half of the ranking. SGD and SGD-momentum perform about equally, with some good results and some bad results. We could not conclude that one architecture is generaly superior to the others.
We conclude that the Adam optimizer with a network structure of [17, 32, 8, 32, 2] neurons, ReLU activation function and a constant learning rate of 0.0001 would perform best.
We therefore set up a new model with the optimal parameters, recombine our training- and validation set, and evaluate our new optimal model against the test set, which is so far not being used.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        Optimizer & Structure & Activation function & Learning Rate & Validation Loss \\ \hline
        Adam & [17, 32, 8, 32, 2] & relu & 0.0001 & 0.00143 \\ \hline
        Adam & [17, 32, 32, 2] & relu & 0.0001 & 0.00153 \\ \hline
        Adam & [17, 8, 8, 2] & relu & 0.0001 & 0.00155 \\ \hline
        Adam & [17, 32, 32, 2] & sigmoid & 0.0001 & 0.00156 \\ \hline
        Adam & [17, 32, 8, 32, 2] & relu & Exp. dec. & 0.00161 \\ \hline
        Adam & [17, 8, 8, 2] & sigmoid & 0.0001 & 0.00165 \\ \hline
        Adam & [17, 32, 8, 32, 2] & sigmoid & 0.0001 & 0.00169 \\ \hline
        Adam & [17, 8, 8, 2] & relu & Exp. dec. & 0.00169 \\ \hline
        SGD-Mom & [17, 8, 8, 2] & sigmoid & 0.0001 & 0.00186 \\ \hline
        Adam & [17, 32, 32, 2] & relu & Exp. dec. & 0.00190 \\ \hline
        Adam & [17, 32, 8, 32, 2] & sigmoid & Exp. dec. & 0.00190 \\ \hline
        SGD-Mom & [17, 32, 8, 32, 2] & sigmoid & 0.0001 & 0.00192 \\ \hline
        Adam & [17, 32, 32, 2] & sigmoid & Exp. dec. & 0.00193 \\ \hline
        SGD-Mom & [17, 32, 32, 2] & sigmoid & Exp. dec. & 0.00201 \\ \hline
        SGD & [17, 32, 8, 32, 2] & sigmoid & 0.0001 & 0.00213 \\ \hline
        SGD & [17, 32, 32, 2] & sigmoid & 0.0001 & 0.00217 \\ \hline
        SGD-Mom & [17, 32, 32, 2] & sigmoid & 0.0001 & 0.00224 \\ \hline
        Adam & [17, 8, 8, 2] & sigmoid & Exp. dec. & 0.00238 \\ \hline
        SGD & [17, 32, 8, 32, 2] & relu & 0.0001 & 0.00255 \\ \hline
        SGD-Mom & [17, 32, 8, 32, 2] & relu & 0.0001 & 0.00258 \\ \hline
        SGD-Mom & [17, 8, 8, 2] & relu & 0.0001 & 0.00316 \\ \hline
        SGD & [17, 32, 32, 2] & relu & 0.0001 & 0.00353 \\ \hline
        SGD-Mom & [17, 32, 8, 32, 2] & sigmoid & Exp. dec. & 0.00362 \\ \hline
        SGD-Mom & [17, 32, 32, 2] & relu & Exp. dec. & 0.00378 \\ \hline
        SGD & [17, 8, 8, 2] & sigmoid & 0.0001 & 0.00385 \\ \hline
        SGD-Mom & [17, 32, 32, 2] & relu & 0.0001 & 0.00390 \\ \hline
        SGD & [17, 8, 8, 2] & relu & 0.0001 & 0.00436 \\ \hline
        SGD-Mom & [17, 32, 8, 32, 2] & relu & Exp. dec. & 0.00461 \\ \hline
        SGD & [17, 32, 8, 32, 2] & sigmoid & Exp. dec. & 0.00699 \\ \hline
        SGD-Mom & [17, 8, 8, 2] & relu & Exp. dec. & 0.00737 \\ \hline
        SGD & [17, 8, 8, 2] & relu & Exp. dec. & 0.00773 \\ \hline
        SGD & [17, 32, 8, 32, 2] & relu & Exp. dec. & 0.01240 \\ \hline
        SGD & [17, 32, 32, 2] & sigmoid & Exp. dec. & 0.01598 \\ \hline
        SGD & [17, 8, 8, 2] & sigmoid & Exp. dec. & 0.01620 \\ \hline
        SGD & [17, 32, 32, 2] & relu & Exp. dec. & 0.03388 \\ \hline
        SGD-Mom & [17, 8, 8, 2] & sigmoid & Exp. dec. & 0.04600 \\ \hline
    \end{tabular}  
    \caption{Overview of hyperparameter combinations with corresponding validation loss}
    \label{table:overview_table}
\end{table}

\subsection*{1.5 Final test evaluation of best setup}
Figure \ref{fig:best_model} shows the training performance for our best model based on previous validations. The best training loss achieved on the combined training set was 0.0016, while the final test-loss was 0.00192. This suggests that there is a slight bit of overfitting, which we find not to serious to require further counter meassures. If overfitting would become too strong, regularization techniques could be employed, for example L1 or L2 regularization.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{training_evolution_best_model.png}  
    \caption{Training loss for the best performing model}
    \label{fig:best_model}
\end{figure}




Figure \ref{fig:real_vs_pred} shows how the predictions of the best model compare to the real world values. We sorted the real world values from smallest to largest for better visibility. Ideally, a line formed by the real world values would be accompanied by a similar line from the predictions. The predictions coming from our best performing model do not follow the real world values closely, instead, the model appears to guess values that are around the mean value of the real values. In case of the volunteering rate, a slight trend following the real values can be seen, but the predictions are still scattered wildly around the the real values. In case of the civic engagement metric, the predictions do not seem to follow any trend, and appear to be randomly scattered around the mean, suggesting that the model is still not able to properly predict these metrics.
It is also apparent that most of the real world data is within a small interval of values. A different normalization technique that spreads values better between the intended interval [0..1] might allow the model to better fit to the data.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{prediction_vs_real_civic_organizations_zip.png}  
    \caption{Predictions by best performing model, compared to real values}
    \label{fig:real_vs_pred}
\end{figure}